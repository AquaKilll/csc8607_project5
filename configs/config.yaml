# Fichier de configuration générique (à adapter au projet)
dataset:
  name: IMDB                # ex: "CIFAR10", "IMDB", ...
  root: "./data"            # chemin local pour les données
  split:                    # optionnel : noms/tailles de splits
    train: 0.8
    val: 0.2
    test: "test"  # IMDb fournit déjà un split test
  download: true            # si supporté
  num_workers: 2
  shuffle: true
  max_length: 256  # Longueur maximale des séquences
  vocab_size: 10000  # Taille du vocabulaire


preprocess:
  # transformations de base (ex: resize, normalize)
  resize:                   # ex: [32, 32] ou null
  normalize:                # ex: {"mean": [0.5,0.5,0.5], "std":[0.5,0.5,0.5]} ou null
  text_tokenizer:           # pour NLP : nom ou paramètres, sinon null

augment:
  # data augmentation (laisser null si non utilisée)
  random_flip:              # true/false
  random_crop:              # paramètres ou null
  color_jitter:             # paramètres ou null
  spec_augment:             # pour audio : paramètres ou null

model:
  type: BiLSTM_Attention    # ex: "resnet18", "mlp", "lstm", ...
  num_classes: 1
  input_shape: 256          # ex: [3, 32, 32] ou null
  embedding_dim: 100
  hidden_sizes: 128            # ex: [256, 128] ou null
  activation: tanh          # ex: relu, gelu, tanh...
  num_layers: 2
  dropout: 0.3             # ex: 0.0–0.5
  bidirectional: true
  batch_norm: false         # true/false
  residual: false           # true/false
  attention: true          # true/false

train:
  seed: 42
  device: auto              # "cpu", "cuda", ou "auto"
  batch_size: 64
  epochs: 10
  max_steps: null           # entier ou null
  overfit_small: false      # true pour sur-apprendre sur un petit échantillon

  optimizer:
    name: adam              # sgd/adam/rmsprop
    lr: 0.001
    weight_decay: 0.0001
    momentum: 0.9           # utile si SGD

  scheduler:
    name: none              # none/step/cosine/onecycle
    step_size: 10
    gamma: 0.1
    warmup_steps: 0

metrics:
  classification:           # ex: ["accuracy", "f1"]
    - accuracy
    - f1
  regression: []            # ex: ["mae", "rmse"]

hparams:                    # espace pour mini grid search
  lr: [0.0005, 0.001, 0.002]
  batch_size: [32, 64]
  weight_decay: [0.00001, 0.0001]
  num_layers: [1, 2]

paths:
  runs_dir: "./runs"
  artifacts_dir: "./artifacts"